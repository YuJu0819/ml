{"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"hw7_macbert_post-proces","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"10d0f6c2ce434ca39368ee84a74cb0c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"156089d6b4154cf5b7e3e5ab57c6ca71":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"1c1fde95b0514c2386d23562c6e74111":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"1de3e29e090a472f99045df4db561910":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fd84584280eb48edb6c95d7af687778a","IPY_MODEL_2dfa673b5c30405b89ff2939b7de2c52"],"layout":"IPY_MODEL_f6eb141d6ffe4b8aaac8e9f76759ef26"}},"1df37e6af4184e478d28bc1e426997c3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_7dea96acd33c4f1f96fdc33a35610903","max":3367,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e3ac97577b054f9cb424cea593ce1d06","value":3367}},"22912102b8d64c58a811698e46466034":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5f56fdb8cbf649a8bfc5488a52e1875b","IPY_MODEL_cf4f7857c8e547ff92922962b5d8c72d"],"layout":"IPY_MODEL_6e506deb5f594ae59c017e6af2174653"}},"26af0fe4453c40988cb1223ade37b667":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"298e48e35f2549a78a9c913cef621f71":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29ad889789254729adaf624fe1436e83":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_46e9e1b2e88c467f8cec4d943ed59f63","placeholder":"​","style":"IPY_MODEL_3cfbd85119d24c2980598a388a48ffe1","value":" 3524/3524 [01:59&lt;00:00, 29.59it/s]"}},"2dfa673b5c30405b89ff2939b7de2c52":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_32634ac4777f44c989a6f89277b2ac2d","placeholder":"​","style":"IPY_MODEL_90beb800a68f43b8acb5c8e778159da4","value":" 3524/3524 [01:58&lt;00:00, 29.64it/s]"}},"32634ac4777f44c989a6f89277b2ac2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37e63fdf92f84758bedb1c0553048576":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3cfbd85119d24c2980598a388a48ffe1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"43e30460face4b9585e3f4d5e0a83d12":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46e9e1b2e88c467f8cec4d943ed59f63":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49395392dd304204b9fd9d390edc0831":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"561daaeffedc4279aa82de07c32d57dc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_49395392dd304204b9fd9d390edc0831","placeholder":"​","style":"IPY_MODEL_37e63fdf92f84758bedb1c0553048576","value":" 3524/3524 [01:59&lt;00:00, 29.40it/s]"}},"5f56fdb8cbf649a8bfc5488a52e1875b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_97d4d92a577f460e81090b812679e877","max":3367,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b711afd927b54033bb026b6526325162","value":3367}},"61a8295903b3444098f007b97a29eead":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"69cea7d8b8f0469aa2d25564a1f8e2c9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"6d2d8b2f9926414ca34a48b2fd2964a6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_26af0fe4453c40988cb1223ade37b667","max":3367,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9f094c22470d48eb892ee90ca667e4bf","value":3367}},"6e506deb5f594ae59c017e6af2174653":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e871b8843934b13adeea4bb39385e24":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_957ad0fef1584ff4829095987f93911e","max":3493,"min":0,"orientation":"horizontal","style":"IPY_MODEL_61a8295903b3444098f007b97a29eead","value":3493}},"720949528cb9469d95994085b6aca532":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7361adfa6d1447b0ac0f5235289da59b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"746ec3f80a7d48cbba6a96381e4d142b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d9721459b884f03a12212dd8a49c89c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_298e48e35f2549a78a9c913cef621f71","placeholder":"​","style":"IPY_MODEL_e28a991943ba490ab34d2b4bf6530d41","value":" 3367/3367 [16:25&lt;00:00,  3.42it/s]"}},"7dea96acd33c4f1f96fdc33a35610903":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f48e3798adf463894c797836393fa2a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"877c29b1c39f4573bd98e567633c1781":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"891e658421564a15acec55785f2348c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"894a1cd4b531407d8eb8b791d954e041":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_e66e977492df469a988c79682038d043","max":3524,"min":0,"orientation":"horizontal","style":"IPY_MODEL_156089d6b4154cf5b7e3e5ab57c6ca71","value":3524}},"90beb800a68f43b8acb5c8e778159da4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"937562a8d36b4ae995d5ff2f9dc4eb4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"957ad0fef1584ff4829095987f93911e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"962b5412f044440f87727b9654de4676":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_894a1cd4b531407d8eb8b791d954e041","IPY_MODEL_561daaeffedc4279aa82de07c32d57dc"],"layout":"IPY_MODEL_dd1f06dd97f5430c8338ebfd34778df5"}},"97d4d92a577f460e81090b812679e877":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b68888a7b194083a274b90204983924":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ba2c0d1913bb44b495e58d68552b3b17","IPY_MODEL_29ad889789254729adaf624fe1436e83"],"layout":"IPY_MODEL_746ec3f80a7d48cbba6a96381e4d142b"}},"9d990d0b19984b6ea05fad011f12a268":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6d2d8b2f9926414ca34a48b2fd2964a6","IPY_MODEL_cc5df39b097f45c395e161a2f45d9774"],"layout":"IPY_MODEL_e72d9562859c4086ae4e89fffdd88ea1"}},"9e0fb7ca24e74a2fbd9a886cb1241297":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1df37e6af4184e478d28bc1e426997c3","IPY_MODEL_7d9721459b884f03a12212dd8a49c89c"],"layout":"IPY_MODEL_877c29b1c39f4573bd98e567633c1781"}},"9f094c22470d48eb892ee90ca667e4bf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"b711afd927b54033bb026b6526325162":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"ba2c0d1913bb44b495e58d68552b3b17":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_43e30460face4b9585e3f4d5e0a83d12","max":3524,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1c1fde95b0514c2386d23562c6e74111","value":3524}},"bd6ab0e2579446aeb3649429358d2896":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c1fbaa249ff941f89afb8f0d1b53c3c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc5df39b097f45c395e161a2f45d9774":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_720949528cb9469d95994085b6aca532","placeholder":"​","style":"IPY_MODEL_bd6ab0e2579446aeb3649429358d2896","value":" 3367/3367 [16:25&lt;00:00,  3.42it/s]"}},"cf4f7857c8e547ff92922962b5d8c72d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_937562a8d36b4ae995d5ff2f9dc4eb4e","placeholder":"​","style":"IPY_MODEL_7361adfa6d1447b0ac0f5235289da59b","value":" 3367/3367 [16:25&lt;00:00,  3.42it/s]"}},"dd1f06dd97f5430c8338ebfd34778df5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e28a991943ba490ab34d2b4bf6530d41":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3ac97577b054f9cb424cea593ce1d06":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"e3fe9293168642aa8117ebd2701b2111":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f48e3798adf463894c797836393fa2a","placeholder":"​","style":"IPY_MODEL_891e658421564a15acec55785f2348c7","value":" 3493/3493 [01:58&lt;00:00, 29.55it/s]"}},"e66e977492df469a988c79682038d043":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e72d9562859c4086ae4e89fffdd88ea1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6eb141d6ffe4b8aaac8e9f76759ef26":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb2ea040a1dc453cad50db782cadc1a2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6e871b8843934b13adeea4bb39385e24","IPY_MODEL_e3fe9293168642aa8117ebd2701b2111"],"layout":"IPY_MODEL_10d0f6c2ce434ca39368ee84a74cb0c4"}},"fd84584280eb48edb6c95d7af687778a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_c1fbaa249ff941f89afb8f0d1b53c3c8","max":3524,"min":0,"orientation":"horizontal","style":"IPY_MODEL_69cea7d8b8f0469aa2d25564a1f8e2c9","value":3524}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Homework 7 - Bert (Question Answering)**\n\nIf you have any questions, feel free to email us at ntu-ml-2021spring-ta@googlegroups.com\n\n\n\nSlide:    [Link](https://docs.google.com/presentation/d/1aQoWogAQo_xVJvMQMrGaYiWzuyfO0QyLLAhiMwFyS2w)　Kaggle: [Link](https://www.kaggle.com/c/ml2021-spring-hw7)　Data: [Link](https://drive.google.com/uc?id=1znKmX08v9Fygp-dgwo7BKiLIf2qL1FH1)\n\n\n","metadata":{"id":"xvSGDbExff_I"}},{"cell_type":"markdown","source":"## Task description\n- Chinese Extractive Question Answering\n  - Input: Paragraph + Question\n  - Output: Answer\n\n- Objective: Learn how to fine tune a pretrained model on downstream task using transformers\n\n- Todo\n    - Fine tune a pretrained chinese BERT model\n    - Change hyperparameters (e.g. doc_stride)\n    - Apply linear learning rate decay\n    - Try other pretrained models\n    - Improve preprocessing\n    - Improve postprocessing\n- Training tips\n    - Automatic mixed precision\n    - Gradient accumulation\n    - Ensemble\n\n- Estimated training time (tesla t4 with automatic mixed precision enabled)\n    - Simple: 8mins\n    - Medium: 8mins\n    - Strong: 25mins\n    - Boss: 2hrs\n  ","metadata":{"id":"WGOr_eS3wJJf"}},{"cell_type":"markdown","source":"## Download Dataset","metadata":{"id":"TJ1fSAJE2oaC"}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kRT9I2f3-ewi","outputId":"3c58e04a-08f5-492e-9a96-8769764fb0c6","execution":{"iopub.status.busy":"2023-05-02T17:45:38.682623Z","iopub.execute_input":"2023-05-02T17:45:38.683074Z","iopub.status.idle":"2023-05-02T17:45:39.853416Z","shell.execute_reply.started":"2023-05-02T17:45:38.683032Z","shell.execute_reply":"2023-05-02T17:45:39.852057Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Tue May  2 17:45:39 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   36C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kFO9z-sWrpxl","outputId":"f00ee25f-88e9-490b-af4e-20580d967118"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"AMklbkD-YO6A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Download link 1\n#!gdown --id '1znKmX08v9Fygp-dgwo7BKiLIf2qL1FH1' --output hw7_data.zip\n\n# Download Link 2 (if the above link fails) \n# !gdown --id '1pOu3FdPdvzielUZyggeD7KDnVy9iW1uC' --output hw7_data.zip\n\n#!unzip -o hw7_data.zip\n\n# For this HW, K80 < P4 < T4 < P100 <= T4(fp16) < V100\n#!nvidia-smi","metadata":{"id":"YPrc4Eie9Yo5","execution":{"iopub.status.busy":"2023-05-02T17:45:39.856389Z","iopub.execute_input":"2023-05-02T17:45:39.857099Z","iopub.status.idle":"2023-05-02T17:45:39.864868Z","shell.execute_reply.started":"2023-05-02T17:45:39.857048Z","shell.execute_reply":"2023-05-02T17:45:39.863746Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Install transformers\n\nDocumentation for the toolkit:　https://huggingface.co/transformers/","metadata":{"id":"TevOvhC03m0h"}},{"cell_type":"code","source":"# You are allowed to change version of transformers or use other toolkits\n!pip install transformers==4.26.1\n!pip install accelerate==0.16.0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tbxWFX_jpDom","outputId":"a1bd862f-c7dc-4356-823a-4601666c0df5","execution":{"iopub.status.busy":"2023-05-03T06:42:48.110482Z","iopub.execute_input":"2023-05-03T06:42:48.110827Z","iopub.status.idle":"2023-05-03T06:43:15.376678Z","shell.execute_reply.started":"2023-05-03T06:42:48.110799Z","shell.execute_reply":"2023-05-03T06:43:15.375519Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting transformers==4.26.1\n  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1) (2.28.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1) (2023.3.23)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1) (6.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1) (0.13.4)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1) (4.64.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1) (0.13.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1) (3.11.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1) (21.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1) (1.23.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.1) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.26.1) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.26.1) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.26.1) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.26.1) (2022.12.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.26.1) (2.1.1)\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.28.1\n    Uninstalling transformers-4.28.1:\n      Successfully uninstalled transformers-4.28.1\nSuccessfully installed transformers-4.26.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting accelerate==0.16.0\n  Downloading accelerate-0.16.0-py3-none-any.whl (199 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.16.0) (5.9.4)\nRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.16.0) (2.0.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.16.0) (6.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.16.0) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.16.0) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.16.0) (3.0.9)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.16.0) (3.1.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.16.0) (3.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.16.0) (4.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.16.0) (3.11.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.16.0) (1.11.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->accelerate==0.16.0) (2.1.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->accelerate==0.16.0) (1.3.0)\nInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.12.0\n    Uninstalling accelerate-0.12.0:\n      Successfully uninstalled accelerate-0.12.0\nSuccessfully installed accelerate-0.16.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"## Import Packages","metadata":{"id":"8dKM4yCh4LI_"}},{"cell_type":"code","source":"import json\nimport numpy as np\nimport random\nimport torch\nfrom torch.utils.data import DataLoader, Dataset \nfrom transformers import AdamW, BertForQuestionAnswering, BertTokenizer, get_linear_schedule_with_warmup, BertModel, BertTokenizerFast\n\nfrom tqdm.auto import tqdm\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Fix random seed for reproducibility\ndef same_seeds(seed):\n\t  torch.manual_seed(seed)\n\t  if torch.cuda.is_available():\n\t\t    torch.cuda.manual_seed(seed)\n\t\t    torch.cuda.manual_seed_all(seed)\n\t  np.random.seed(seed)\n\t  random.seed(seed)\n\t  torch.backends.cudnn.benchmark = False\n\t  torch.backends.cudnn.deterministic = True\nsame_seeds(14)","metadata":{"id":"WOTHHtWJoahe","execution":{"iopub.status.busy":"2023-05-03T08:45:31.859431Z","iopub.execute_input":"2023-05-03T08:45:31.859835Z","iopub.status.idle":"2023-05-03T08:45:42.712854Z","shell.execute_reply.started":"2023-05-03T08:45:31.859804Z","shell.execute_reply":"2023-05-03T08:45:42.711939Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Change \"fp16_training\" to True to support automatic mixed precision training (fp16)\t\nfp16_training = True\n\nif fp16_training:\n#     !pip install accelerate==0.2.0\n    from accelerate import Accelerator\n    accelerator = Accelerator(mixed_precision=\"fp16\")\n#     device = accelerator.device\n\n# Documentation for the toolkit:  https://huggingface.co/docs/accelerate/","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7pBtSZP1SKQO","outputId":"c372f903-2989-4909-d56e-1aede9079c3d","execution":{"iopub.status.busy":"2023-05-03T08:45:47.379520Z","iopub.execute_input":"2023-05-03T08:45:47.380156Z","iopub.status.idle":"2023-05-03T08:45:47.384522Z","shell.execute_reply.started":"2023-05-03T08:45:47.380124Z","shell.execute_reply":"2023-05-03T08:45:47.383663Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Load Model and Tokenizer\n\n\n\n\n ","metadata":{"id":"2YgXHuVLp_6j"}},{"cell_type":"code","source":"tokenizer = BertTokenizerFast.from_pretrained(\"hfl/chinese-pert-large-mrc\")\nmodel = BertForQuestionAnswering.from_pretrained(\"hfl/chinese-pert-large-mrc\").to(device)\n\n# You can safely ignore the warning message (it pops up because new prediction heads for QA are initialized randomly)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xyBCYGjAp3ym","outputId":"da6b6561-a8f8-4405-e2bf-2355b5fd1ea2","execution":{"iopub.status.busy":"2023-05-02T17:46:20.022660Z","iopub.execute_input":"2023-05-02T17:46:20.022934Z","iopub.status.idle":"2023-05-02T17:46:38.874846Z","shell.execute_reply.started":"2023-05-02T17:46:20.022908Z","shell.execute_reply":"2023-05-02T17:46:38.873824Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/19.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08c2c490fd3246fabf48dbd60680c745"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75f6d2a93a6d4601839f5ea671e3b25e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebbc055fcc1a4d5292bc9515f77a24fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)in/added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b03ac24a3e64a4f82ac2150426486fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"959a38386ed74385b3f4ceb2ef97b12b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7af3cbf25604fb99595711def4a2bee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/1.31G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1f171a396934b759c3f27dfa09532ad"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at hfl/chinese-macbert-large were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForQuestionAnswering were not initialized from the model checkpoint at hfl/chinese-macbert-large and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Read Data\n\n- Training set: 26935 QA pairs\n- Dev set: 3523  QA pairs\n- Test set: 3492  QA pairs\n\n- {train/dev/test}_questions:\t\n  - List of dicts with the following keys:\n   - id (int)\n   - paragraph_id (int)\n   - question_text (string)\n   - answer_text (string)\n   - answer_start (int)\n   - answer_end (int)\n- {train/dev/test}_paragraphs: \n  - List of strings\n  - paragraph_ids in questions correspond to indexs in paragraphs\n  - A paragraph may be used by several questions ","metadata":{"id":"3Td-GTmk5OW4"}},{"cell_type":"code","source":"def read_data(file):\n    with open(file, 'r', encoding=\"utf-8\") as reader:\n        data = json.load(reader)\n    return data[\"questions\"], data[\"paragraphs\"]\n\ntrain_questions, train_paragraphs = read_data(\"/kaggle/input/ml2023spring-hw7/hw7_train.json\")\ndev_questions, dev_paragraphs = read_data(\"/kaggle/input/ml2023spring-hw7/hw7_dev.json\")\ntest_questions, test_paragraphs = read_data(\"/kaggle/input/ml2023spring-hw7/hw7_test.json\")","metadata":{"id":"NvX7hlepogvu","execution":{"iopub.status.busy":"2023-05-02T17:46:38.876625Z","iopub.execute_input":"2023-05-02T17:46:38.877025Z","iopub.status.idle":"2023-05-02T17:46:39.608216Z","shell.execute_reply.started":"2023-05-02T17:46:38.876984Z","shell.execute_reply":"2023-05-02T17:46:39.607115Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(tokenizer.tokenize('✔', add_special_tokens=False))\nprint(tokenizer.tokenize('●', add_special_tokens=False))\nprint(tokenizer.tokenize('✦', add_special_tokens=False))\nprint(tokenizer.tokenize('☺', add_special_tokens=False))\nprint(tokenizer.tokenize('☆', add_special_tokens=False))\n\n# 取代' ' \\u200b \\u200e \\u3000 # 是為了讓tokenize前後index一致\n# 用✔ ● ✦ ☺ ☆ 當佔位符，沒有意義\n#train_paragraphs = [i.replace(' ','✔').replace('\\u200b','✦').replace('\\u200e', '☺').replace('\\u3000', '☆').replace('#','●') for i in train_paragraphs]\ndev_paragraphs = [i.replace(' ','✔').replace('\\u200b','✦').replace('\\u200e', '☺').replace('\\u3000', '☆').replace('#','●') for i in dev_paragraphs]\ntest_paragraphs = [i.replace(' ','✔').replace('\\u200b','✦').replace('\\u200e', '☺').replace('\\u3000', '☆').replace('#','●') for i in test_paragraphs]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3FgKuiV3gnoe","outputId":"e11d66ce-78fd-475f-eb0a-4e01779482a4","execution":{"iopub.status.busy":"2023-05-02T17:46:39.609785Z","iopub.execute_input":"2023-05-02T17:46:39.610179Z","iopub.status.idle":"2023-05-02T17:46:41.381442Z","shell.execute_reply.started":"2023-05-02T17:46:39.610137Z","shell.execute_reply":"2023-05-02T17:46:41.380299Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"['✔']\n['●']\n['✦']\n['☺']\n['☆']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Tokenize Data","metadata":{"id":"Fm0rpTHq0e4N"}},{"cell_type":"code","source":"# Tokenize questions and paragraphs separately\n# 「add_special_tokens」 is set to False since special tokens will be added when tokenized questions and paragraphs are combined in datset __getitem__ \n\ntrain_questions_tokenized = tokenizer([train_question[\"question_text\"] for train_question in train_questions], add_special_tokens=False)\ndev_questions_tokenized = tokenizer([dev_question[\"question_text\"] for dev_question in dev_questions], add_special_tokens=False)\ntest_questions_tokenized = tokenizer([test_question[\"question_text\"] for test_question in test_questions], add_special_tokens=False) \n\ntrain_paragraphs_tokenized = tokenizer(train_paragraphs, add_special_tokens=False)\ndev_paragraphs_tokenized = tokenizer(dev_paragraphs, add_special_tokens=False)\ntest_paragraphs_tokenized = tokenizer(test_paragraphs, add_special_tokens=False)\n\n# You can safely ignore the warning message as tokenized sequences will be futher processed in datset __getitem__ before passing to model","metadata":{"id":"rTZ6B70Hoxie","execution":{"iopub.status.busy":"2023-05-02T17:46:41.382852Z","iopub.execute_input":"2023-05-02T17:46:41.383330Z","iopub.status.idle":"2023-05-02T17:46:56.467340Z","shell.execute_reply.started":"2023-05-02T17:46:41.383289Z","shell.execute_reply":"2023-05-02T17:46:56.466234Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Dataset and Dataloader","metadata":{"id":"Ws8c8_4d5UCI"}},{"cell_type":"code","source":"class QA_Dataset(Dataset):\n    def __init__(self, split, questions, tokenized_questions, tokenized_paragraphs):\n        self.split = split\n        self.questions = questions\n        self.tokenized_questions = tokenized_questions\n        self.tokenized_paragraphs = tokenized_paragraphs\n        self.max_question_len = 80\n        self.max_paragraph_len = 350\n        \n        ##### TODO: Change value of doc_stride #####\n        self.doc_stride = 300\n\n        # Input sequence length = [CLS] + question + [SEP] + paragraph + [SEP]\n        self.max_seq_len = 1 + self.max_question_len + 1 + self.max_paragraph_len + 1\n\n    def __len__(self):\n        return len(self.questions)\n\n    def __getitem__(self, idx):\n        question = self.questions[idx]\n        tokenized_question = self.tokenized_questions[idx]\n        tokenized_paragraph = self.tokenized_paragraphs[question[\"paragraph_id\"]]\n\n        ##### TODO: Preprocessing #####\n        # Hint: How to prevent model from learning something it should not learn\n\n        if self.split == \"train\":\n            # Convert answer's start/end positions in paragraph_text to start/end positions in tokenized_paragraph  \n            answer_start_token = tokenized_paragraph.char_to_token(question[\"answer_start\"])\n            answer_end_token = tokenized_paragraph.char_to_token(question[\"answer_end\"])\n\n            # A single window is obtained by slicing the portion of paragraph containing the answer\n            mid = int((answer_start_token + answer_end_token) // (2+random.uniform(-1,1)))\n            paragraph_start = max(0, min(mid - self.max_paragraph_len // 2, len(tokenized_paragraph) - self.max_paragraph_len))\n            #if answer_start_token >  self.max_paragraph_len:\n                # paragraph_start 不能從0開始\n            #    paragraph_start = min(answer_start_token - self.max_paragraph_len // 2, len(tokenized_paragraph) - self.max_paragraph_len)\n            #else:\n            #    paragraph_start = 0\n            paragraph_end = paragraph_start + self.max_paragraph_len\n            \n\n            # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n            input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102] \n            input_ids_paragraph = tokenized_paragraph.ids[paragraph_start : paragraph_end] + [102]\t\t\n            \n            # Convert answer's start/end positions in tokenized_paragraph to start/end positions in the window  \n            answer_start_token += len(input_ids_question) - paragraph_start\n            answer_end_token += len(input_ids_question) - paragraph_start\n            \n            # Pad sequence and obtain inputs to model \n            input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n            return torch.tensor(input_ids), torch.tensor(token_type_ids), torch.tensor(attention_mask), answer_start_token, answer_end_token\n\n        # Validation/Testing\n        else:\n            input_ids_list, token_type_ids_list, attention_mask_list = [], [], []\n            \n            # Paragraph is split into several windows, each with start positions separated by step \"doc_stride\"\n            for i in range(0, len(tokenized_paragraph), self.doc_stride):\n                \n                # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n                input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102]\n                input_ids_paragraph = tokenized_paragraph.ids[i : i + self.max_paragraph_len] + [102]\n                \n                # Pad sequence and obtain inputs to model\n                input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n                \n                input_ids_list.append(input_ids)\n                token_type_ids_list.append(token_type_ids)\n                attention_mask_list.append(attention_mask)\n            \n            return torch.tensor(input_ids_list), torch.tensor(token_type_ids_list), torch.tensor(attention_mask_list)\n\n    def padding(self, input_ids_question, input_ids_paragraph):\n        # Pad zeros if sequence length is shorter than max_seq_len\n        padding_len = self.max_seq_len - len(input_ids_question) - len(input_ids_paragraph)\n        # Indices of input sequence tokens in the vocabulary\n        input_ids = input_ids_question + input_ids_paragraph + [0] * padding_len\n        # Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]\n        token_type_ids = [0] * len(input_ids_question) + [1] * len(input_ids_paragraph) + [0] * padding_len\n        # Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]\n        attention_mask = [1] * (len(input_ids_question) + len(input_ids_paragraph)) + [0] * padding_len\n        \n        return input_ids, token_type_ids, attention_mask\n\ntrain_set = QA_Dataset(\"train\", train_questions, train_questions_tokenized, train_paragraphs_tokenized)\ndev_set = QA_Dataset(\"train\", dev_questions, dev_questions_tokenized, dev_paragraphs_tokenized)\ntest_set = QA_Dataset(\"test\", test_questions, test_questions_tokenized, test_paragraphs_tokenized)\n\ntrain_batch_size = 8\n# train_set = torch.utils.data.ConcatDataset([train_set, dev_set])\n# Note: Do NOT change batch size of dev_loader / test_loader !\n# Although batch size=1, it is actually a batch consisting of several windows from the same QA pair\ntrain_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True, pin_memory=True)\ndev_loader = DataLoader(dev_set, batch_size=1, shuffle=False, pin_memory=True)\ntest_loader = DataLoader(test_set, batch_size=1, shuffle=False, pin_memory=True)","metadata":{"id":"Xjooag-Swnuh","execution":{"iopub.status.busy":"2023-05-02T17:46:56.471838Z","iopub.execute_input":"2023-05-02T17:46:56.472450Z","iopub.status.idle":"2023-05-02T17:46:56.491625Z","shell.execute_reply.started":"2023-05-02T17:46:56.472411Z","shell.execute_reply":"2023-05-02T17:46:56.490580Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Function for Evaluation","metadata":{"id":"5_H1kqhR8CdM"}},{"cell_type":"code","source":"def index_before_tokenize(tokens, start, end):\n    char_count, new_start, new_end = 0, 512, 512\n    start_flag = 0\n    end_flag = 0\n        \n    for i, token in enumerate(tokens):\n        if token == '[UNK]' or token == '[CLS]' or token == '[SEP]':\n            if i == start:\n                new_start = char_count\n            if i == end:\n                new_end = char_count\n            char_count += 1\n        else:\n            for c in token:\n                if i == start and start_flag == 0:\n                    #print(token)\n                    new_start = char_count\n                    start_flag = 1\n                if i == end:\n                    #print(token)\n                    new_end = char_count\n                    end_flag = 1\n                if c != '#':\n                    char_count += 1\n    return new_start, new_end","metadata":{"id":"nqAQYOsfhL5c","execution":{"iopub.status.busy":"2023-05-02T17:46:56.493200Z","iopub.execute_input":"2023-05-02T17:46:56.493848Z","iopub.status.idle":"2023-05-02T17:46:56.511637Z","shell.execute_reply.started":"2023-05-02T17:46:56.493810Z","shell.execute_reply":"2023-05-02T17:46:56.510557Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def evaluate(data, output, doc_stride=300, paragraph=None, paragraph_tokenized=None):\n    ##### TODO: Postprocessing #####\n    # There is a bug and room for improvement in postprocessing \n    # Hint: Open your prediction file to see what is wrong \n    \n    answer = ''\n    max_prob = float('-inf')\n    num_of_windows = data[0].shape[1]\n    \n    # index in the whole tokens (not just relative to window)\n    entire_start_index = 0\n    entire_end_index = 0\n    \n    for k in range(num_of_windows):\n        #print('window',k)\n        # Obtain answer by choosing the most probable start position / end position\n        mask = data[1][0][k].bool() & data[2][0][k].bool() # token type & attention mask\n        masked_output_start = torch.masked_select(output.start_logits[k], mask)[:-1] # -1 is [SEP]\n        start_prob, start_index = torch.max(masked_output_start, dim=0)\n        masked_output_end = torch.masked_select(output.end_logits[k], mask)[start_index:-1] # -1 is [SEP]\n        #masked_output_end = torch.masked_select(output.end_logits[k], mask)[:-1] # -1 is [SEP]\n        end_prob, end_index = torch.max(masked_output_end, dim=0)\n        end_index += start_index \n        \n\n        # Probability of answer is calculated as sum of start_prob and end_prob\n        prob = start_prob + end_prob\n        masked_data = torch.masked_select(data[0][0][k], mask)[:-1] # -1 is [SEP]\n\n        # Replace answer if calculated probability is larger than previous windows\n        if (prob > max_prob) and (end_index - start_index <= 30) and (end_index > start_index):\n            max_prob = prob\n            entire_start_index = start_index.item() + doc_stride * k\n            entire_end_index = end_index.item() + doc_stride * k\n            #print('entire_start_index',entire_start_index)\n            #print('entire_end_index',entire_end_index)\n            # Convert tokens to chars (e.g. [1920, 7032] --> \"大 金\")\n            answer = tokenizer.decode(masked_data[start_index : end_index + 1])\n            # Remove spaces in answer (e.g. \"大 金\" --> \"大金\")\n            answer = answer.replace('✔', ' ').replace('✦','\\u200b').replace('☺','\\u200e').replace('☆','\\u3000').replace('●','#').replace(' ','')\n\n    \n    # if [UNK] in prediction, use orignal span of paragrah\n    if '[UNK]' in answer:\n        print('found [UNK] in prediction, using original text')\n        print('original prediction', answer)\n        # find the index of answer in the orinal paragrah\n\n        new_start, new_end = index_before_tokenize(tokens=paragraph_tokenized, \n                                                   start=entire_start_index, end=entire_end_index)\n        #print('new_start',new_start)\n        #print('new_end',new_end)\n        answer = paragraph[new_start:new_end+1]\n        answer = answer.replace('✔', ' ').replace('✦','\\u200b').replace('☺','\\u200e').replace('☆','\\u3000').replace('●','#')\n        print('final prediction',answer)\n\n            \n    \n    return answer","metadata":{"id":"SqeA3PLPxOHu","execution":{"iopub.status.busy":"2023-05-02T17:46:56.513215Z","iopub.execute_input":"2023-05-02T17:46:56.513641Z","iopub.status.idle":"2023-05-02T17:46:56.531163Z","shell.execute_reply.started":"2023-05-02T17:46:56.513605Z","shell.execute_reply":"2023-05-02T17:46:56.530156Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{"id":"rzHQit6eMnKG"}},{"cell_type":"code","source":"num_epoch = 1\nvalidation = False\nlogging_step = 100\nlearning_rate = 1e-5\ndoc_stride = 300\n# batch accumulation parameter\n# accum_iter = 2\n\noptimizer = AdamW(model.parameters(), lr=learning_rate)\n\n\nif fp16_training:\n    model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader) \n\n# Total number of training steps\ntotal_steps = len(train_loader) * num_epoch\nprint('total_steps', total_steps)\n# Set up the learning rate scheduler\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps= 100, # Default value\n                                                num_training_steps=total_steps)\n\nif not validation:\n    dev_set = QA_Dataset(\"train\", dev_questions, dev_questions_tokenized, dev_paragraphs_tokenized)\n#     dev_loader = DataLoader(dev_set, batch_size=1, shuffle=False, pin_memory=True)\n    train_set = torch.utils.data.ConcatDataset([train_set, dev_set])\n    train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True, pin_memory=True)\n    \nmodel.train()\n\nprint(\"Start Training ...\")\n\nfor epoch in range(num_epoch):\n    step = 1\n    train_loss = train_acc = 0\n    \n    for idx, data in enumerate(tqdm(train_loader)):\t\n        # Load all data into GPU\n        data = [i.to(device) for i in data]\n        \n        # Model inputs: input_ids, token_type_ids, attention_mask, start_positions, end_positions (Note: only \"input_ids\" is mandatory)\n        # Model outputs: start_logits, end_logits, loss (return when start_positions/end_positions are provided)  \n        output = model(input_ids=data[0], token_type_ids=data[1], attention_mask=data[2], start_positions=data[3], end_positions=data[4])\n\n\n        # Choose the most probable start position / end position\n        start_index = torch.argmax(output.start_logits, dim=1)\n        end_index = torch.argmax(output.end_logits, dim=1)\n        \n        # Prediction is correct only if both start_index and end_index are correct\n        train_acc += ((start_index == data[3]) & (end_index == data[4])).float().mean()\n        train_loss += output.loss\n        \n        # normalize loss to account for batch accumulation\n#         output.loss = output.loss / accum_iter\n\n        if fp16_training:\n            accelerator.backward(output.loss)\n        else:\n            output.loss.backward()\n        \n#         if ((idx + 1) % accum_iter == 0) or (idx + 1 == len(train_loader)):\n        optimizer.step()\n            ##### TODO: Apply linear learning rate decay #####\n        scheduler.step()\n        optimizer.zero_grad()\n        \n        step += 1\n        \n        \n        # Print training loss and accuracy over past logging step\n        if step % logging_step == 0:\n            print(f\"Epoch {epoch + 1} | Step {step} | loss = {train_loss.item() / logging_step:.3f}, acc = {train_acc / logging_step:.3f}\")\n            train_loss = train_acc = 0\n\n    if validation:\n        print(\"Evaluating Dev Set ...\")\n        model.eval()\n        with torch.no_grad():\n            dev_acc = 0\n            for i, data in enumerate(tqdm(dev_loader)):\n                data = [j.to(device) for j in data]\n                output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n                       attention_mask=data[2].squeeze(dim=0).to(device))\n                # prediction is correct only if answer text exactly matches\n                dev_acc += evaluate(data, output, doc_stride, dev_paragraphs[dev_questions[i]['paragraph_id']],\n                    dev_paragraphs_tokenized[dev_questions[i]['paragraph_id']].tokens) == dev_questions[i][\"answer_text\"]\n            print(f\"Validation | Epoch {epoch + 1} | acc = {dev_acc / len(dev_loader):.3f}\")\n        model.train()\n\n# Save a model and its configuration file to the directory 「saved_model」 \n# i.e. there are two files under the direcory 「saved_model」: 「pytorch_model.bin」 and 「config.json」\n# Saved model can be re-loaded using 「model = BertForQuestionAnswering.from_pretrained(\"saved_model\")」\nprint(\"Saving Model ...\")\nmodel_save_dir = \"saved_model/macbert4\" \nmodel.save_pretrained(model_save_dir)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["22912102b8d64c58a811698e46466034","6e506deb5f594ae59c017e6af2174653","5f56fdb8cbf649a8bfc5488a52e1875b","cf4f7857c8e547ff92922962b5d8c72d","b711afd927b54033bb026b6526325162","97d4d92a577f460e81090b812679e877","7361adfa6d1447b0ac0f5235289da59b","937562a8d36b4ae995d5ff2f9dc4eb4e","1de3e29e090a472f99045df4db561910","f6eb141d6ffe4b8aaac8e9f76759ef26","fd84584280eb48edb6c95d7af687778a","2dfa673b5c30405b89ff2939b7de2c52","69cea7d8b8f0469aa2d25564a1f8e2c9","c1fbaa249ff941f89afb8f0d1b53c3c8","90beb800a68f43b8acb5c8e778159da4","32634ac4777f44c989a6f89277b2ac2d","9e0fb7ca24e74a2fbd9a886cb1241297","877c29b1c39f4573bd98e567633c1781","1df37e6af4184e478d28bc1e426997c3","7d9721459b884f03a12212dd8a49c89c","e3ac97577b054f9cb424cea593ce1d06","7dea96acd33c4f1f96fdc33a35610903","e28a991943ba490ab34d2b4bf6530d41","298e48e35f2549a78a9c913cef621f71","9b68888a7b194083a274b90204983924","746ec3f80a7d48cbba6a96381e4d142b","ba2c0d1913bb44b495e58d68552b3b17","29ad889789254729adaf624fe1436e83","1c1fde95b0514c2386d23562c6e74111","43e30460face4b9585e3f4d5e0a83d12","3cfbd85119d24c2980598a388a48ffe1","46e9e1b2e88c467f8cec4d943ed59f63","9d990d0b19984b6ea05fad011f12a268","e72d9562859c4086ae4e89fffdd88ea1","6d2d8b2f9926414ca34a48b2fd2964a6","cc5df39b097f45c395e161a2f45d9774","9f094c22470d48eb892ee90ca667e4bf","26af0fe4453c40988cb1223ade37b667","bd6ab0e2579446aeb3649429358d2896","720949528cb9469d95994085b6aca532","962b5412f044440f87727b9654de4676","dd1f06dd97f5430c8338ebfd34778df5","894a1cd4b531407d8eb8b791d954e041","561daaeffedc4279aa82de07c32d57dc","156089d6b4154cf5b7e3e5ab57c6ca71","e66e977492df469a988c79682038d043","37e63fdf92f84758bedb1c0553048576","49395392dd304204b9fd9d390edc0831"]},"id":"3Q-B6ka7xoCM","outputId":"f2df1eeb-815f-4d2d-a714-26c597a91781","execution":{"iopub.status.busy":"2023-05-02T17:46:56.533705Z","iopub.execute_input":"2023-05-02T17:46:56.534550Z","iopub.status.idle":"2023-05-02T17:47:06.620106Z","shell.execute_reply.started":"2023-05-02T17:46:56.534513Z","shell.execute_reply":"2023-05-02T17:47:06.616859Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"total_steps 10095\nStart Training ...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3723 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aed4b0c4e7644a9c90e083f6eedceaac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m                                                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m54 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m#output.loss = output.loss / accum_iter\u001b[0m                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m55 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m56 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m fp16_training:                                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m57 \u001b[2m│   │   │   \u001b[0maccelerator.backward(output.loss)                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m58 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m59 \u001b[0m\u001b[2m│   │   │   \u001b[0moutput.loss.backward()                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m60 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.7/site-packages/accelerate/\u001b[0m\u001b[1;33maccelerator.py\u001b[0m:\u001b[94m1439\u001b[0m in \u001b[92mbackward\u001b[0m                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1436 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m \u001b[96mself\u001b[0m.distributed_type == DistributedType.MEGATRON_LM:                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1437 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m                                                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1438 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m \u001b[96mself\u001b[0m.scaler \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1439 \u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.scaler.scale(loss).backward(**kwargs)                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1440 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1441 \u001b[0m\u001b[2m│   │   │   \u001b[0mloss.backward(**kwargs)                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1442 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.7/site-packages/torch/\u001b[0m\u001b[1;33m_tensor.py\u001b[0m:\u001b[94m488\u001b[0m in \u001b[92mbackward\u001b[0m                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 485 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minputs=inputs,                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 486 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 487 \u001b[0m\u001b[2m│   │   \u001b[0mtorch.autograd.backward(                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 488 \u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m, gradient, retain_graph, create_graph, inputs=inputs                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 489 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 490 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 491 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mregister_hook\u001b[0m(\u001b[96mself\u001b[0m, hook):                                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.7/site-packages/torch/autograd/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m199\u001b[0m in \u001b[92mbackward\u001b[0m                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m196 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# calls in the traceback and some print out the last line\u001b[0m                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m197 \u001b[0m\u001b[2m│   \u001b[0mVariable._execution_engine.run_backward(  \u001b[2m# Calls into the C++ engine to run the bac\u001b[0m   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m198 \u001b[0m\u001b[2m│   │   \u001b[0mtensors, grad_tensors_, retain_graph, create_graph, inputs,                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m199 \u001b[2m│   │   \u001b[0mallow_unreachable=\u001b[94mTrue\u001b[0m, accumulate_grad=\u001b[94mTrue\u001b[0m)  \u001b[2m# Calls into the C++ engine to ru\u001b[0m   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m200 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m201 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mgrad\u001b[0m(                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m202 \u001b[0m\u001b[2m│   \u001b[0moutputs: _TensorOrTensors,                                                             \u001b[31m│\u001b[0m\n\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n\u001b[1;91mKeyboardInterrupt\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">54 │   │   #output.loss = output.loss / accum_iter</span>                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">55 │   │   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">56 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> fp16_training:                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>57 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>accelerator.backward(output.loss)                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">58 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">59 │   │   │   </span>output.loss.backward()                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">60 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.7/site-packages/accelerate/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">accelerator.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1439</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1436 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.distributed_type == DistributedType.MEGATRON_LM:                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1437 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span>                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1438 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.scaler <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1439 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.scaler.scale(loss).backward(**kwargs)                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1440 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1441 │   │   │   </span>loss.backward(**kwargs)                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1442 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.7/site-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_tensor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">488</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 485 │   │   │   │   </span>inputs=inputs,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 486 │   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 487 │   │   </span>torch.autograd.backward(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 488 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, gradient, retain_graph, create_graph, inputs=inputs                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 489 │   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 490 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 491 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">register_hook</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, hook):                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.7/site-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">199</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">196 │   # calls in the traceback and some print out the last line</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">197 │   </span>Variable._execution_engine.run_backward(  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to run the bac</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">198 │   │   </span>tensors, grad_tensors_, retain_graph, create_graph, inputs,                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>199 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>allow_unreachable=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, accumulate_grad=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to ru</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">200 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">201 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">grad</span>(                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">202 │   </span>outputs: _TensorOrTensors,                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"## Testing","metadata":{"id":"kMmdLOKBMsdE"}},{"cell_type":"code","source":"# model = BertForQuestionAnswering.from_pretrained(\"/kaggle/input/hw7-boss/saved_model/macbert4\").to(device)","metadata":{"execution":{"iopub.status.busy":"2023-05-02T17:47:10.185416Z","iopub.execute_input":"2023-05-02T17:47:10.185877Z","iopub.status.idle":"2023-05-02T17:47:24.800994Z","shell.execute_reply.started":"2023-05-02T17:47:10.185836Z","shell.execute_reply":"2023-05-02T17:47:24.799908Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(\"Evaluating Test Set ...\")\n\nresult = []\n\nmodel.eval()\nwith torch.no_grad():\n    for i, data in enumerate(tqdm(test_loader)):\n#         data.to(device)\n        data = [j.to(device) for j in data]\n        output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n                       attention_mask=data[2].squeeze(dim=0).to(device))\n        result.append(evaluate(data, output, doc_stride, test_paragraphs[test_questions[i]['paragraph_id']],\n                               test_paragraphs_tokenized[test_questions[i]['paragraph_id']].tokens))\n\nresult_file = \"result_.csv\"\nwith open(result_file, 'w') as f:\t\n\t  f.write(\"ID,Answer\\n\")\n\t  for i, test_question in enumerate(test_questions):\n        # Replace commas in answers with empty strings (since csv is separated by comma)\n        # Answers in kaggle are processed in the same way\n\t\t    f.write(f\"{test_question['id']},{result[i].replace(',','')}\\n\")\n\nprint(f\"Completed! Result is in {result_file}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["fb2ea040a1dc453cad50db782cadc1a2","10d0f6c2ce434ca39368ee84a74cb0c4","6e871b8843934b13adeea4bb39385e24","e3fe9293168642aa8117ebd2701b2111","61a8295903b3444098f007b97a29eead","957ad0fef1584ff4829095987f93911e","891e658421564a15acec55785f2348c7","7f48e3798adf463894c797836393fa2a"]},"id":"U5scNKC9xz0C","outputId":"2e82bc2d-a8af-49ee-e67e-7629458a680a","execution":{"iopub.status.busy":"2023-05-02T17:47:27.634363Z","iopub.execute_input":"2023-05-02T17:47:27.635478Z","iopub.status.idle":"2023-05-02T17:53:58.112053Z","shell.execute_reply.started":"2023-05-02T17:47:27.635433Z","shell.execute_reply":"2023-05-02T17:53:58.110815Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Evaluating Test Set ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3524 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d92691f3c50d45d9beef3a3036b922ec"}},"metadata":{}},{"name":"stdout","text":"found [UNK] in prediction, using original text\noriginal prediction 回[UNK]部落\nfinal prediction 回鶻部落\nfound [UNK] in prediction, using original text\noriginal prediction 回[UNK]汗國\nfinal prediction 回鶻汗國\nfound [UNK] in prediction, using original text\noriginal prediction 白[UNK]紀中期\nfinal prediction 白堊紀中期\nfound [UNK] in prediction, using original text\noriginal prediction 西方式握拍是將「[UNK]」字形虎口對準拍柄的右垂直面，正反手用同一拍面擊球。西方式握拍\nfinal prediction 西方式握拍是將「ㄑ」字形虎口對準拍柄的右垂直面，正反手用同一拍面擊球。西方式握拍\nfound [UNK] in prediction, using original text\noriginal prediction 李[UNK]\nfinal prediction 李勣\nfound [UNK] in prediction, using original text\noriginal prediction 周王是燕王同母兄弟，而朱允[UNK]怕他與燕王呵成一氣\nfinal prediction 周王是燕王同母兄弟，而朱允炆怕他與燕王呵成一氣\nfound [UNK] in prediction, using original text\noriginal prediction 久彌宮妃[UNK]子\nfinal prediction 久彌宮妃俔子\nfound [UNK] in prediction, using original text\noriginal prediction 《阿[UNK]婆吠陀》\nfinal prediction 《阿闥婆吠陀》\nfound [UNK] in prediction, using original text\noriginal prediction 姚[UNK]\nfinal prediction 姚萇\nfound [UNK] in prediction, using original text\noriginal prediction [UNK]水之戰\nfinal prediction 淝水之戰\nfound [UNK] in prediction, using original text\noriginal prediction 金[UNK]\nfinal prediction 金堉\nfound [UNK] in prediction, using original text\noriginal prediction 白[UNK]紀\nfinal prediction 白堊紀\nfound [UNK] in prediction, using original text\noriginal prediction 測試[UNK][UNK]村附近男性居民的脫氧核糖核酸\nfinal prediction 測試驪靬村附近男性居民的脫氧核糖核酸\nfound [UNK] in prediction, using original text\noriginal prediction 美國長[UNK]米\nfinal prediction 美國長秈米\nfound [UNK] in prediction, using original text\noriginal prediction 其中含有[UNK]\nfinal prediction 其中含有烴\nfound [UNK] in prediction, using original text\noriginal prediction [UNK]族\nfinal prediction 撣族\nfound [UNK] in prediction, using original text\noriginal prediction 蔡[UNK]\nfinal prediction 蔡鍔\nfound [UNK] in prediction, using original text\noriginal prediction 白[UNK]紀\nfinal prediction 白堊紀\nfound [UNK] in prediction, using original text\noriginal prediction 大肚平埔族拍布拉族大肚王與瑯[UNK]番人的反抗\nfinal prediction 大肚平埔族拍布拉族大肚王與瑯嶠番人的反抗\nfound [UNK] in prediction, using original text\noriginal prediction 《阿[UNK]婆吠陀》\nfinal prediction 《阿闥婆吠陀》\nfound [UNK] in prediction, using original text\noriginal prediction 朱允[UNK]\nfinal prediction 朱允炆\nfound [UNK] in prediction, using original text\noriginal prediction 盤[UNK]蠻\nfinal prediction 盤瓠蠻\nfound [UNK] in prediction, using original text\noriginal prediction [UNK]靼\nfinal prediction 韃靼\nfound [UNK] in prediction, using original text\noriginal prediction 3000[UNK]顆\nfinal prediction 3000垓顆\nfound [UNK] in prediction, using original text\noriginal prediction 趙[UNK]\nfinal prediction 趙竑\nCompleted! Result is in result_.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Kaggle public: 0.85526","metadata":{}},{"cell_type":"code","source":"def clean_text(text):\n    # Note: When you use unilingual model, the colon may become fullwidth\n    text = text.split(\"答案:\")[-1]\n    text = text.split(\" \")[0]\n    return text","metadata":{"execution":{"iopub.status.busy":"2023-05-03T08:46:00.639299Z","iopub.execute_input":"2023-05-03T08:46:00.639727Z","iopub.status.idle":"2023-05-03T08:46:00.649132Z","shell.execute_reply.started":"2023-05-03T08:46:00.639692Z","shell.execute_reply":"2023-05-03T08:46:00.648145Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\n\n# To avoid CUDA_OUT_OF_MEMORY\ntorch.set_default_tensor_type(torch.cuda.FloatTensor)\n\n# Fix random seed for reproducibility\ndef same_seeds(seed):\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\nsame_seeds(2)","metadata":{"execution":{"iopub.status.busy":"2023-05-03T08:46:04.056624Z","iopub.execute_input":"2023-05-03T08:46:04.056968Z","iopub.status.idle":"2023-05-03T08:46:04.064516Z","shell.execute_reply.started":"2023-05-03T08:46:04.056939Z","shell.execute_reply":"2023-05-03T08:46:04.063585Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\n\n# You can try model with different size\n# When using Colab or Kaggle, model with more than 2 billions parameters may \n# run out of memory\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/xglm-1.7B\")\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/xglm-1.7B\")","metadata":{"execution":{"iopub.status.busy":"2023-05-03T08:46:11.199393Z","iopub.execute_input":"2023-05-03T08:46:11.199729Z","iopub.status.idle":"2023-05-03T08:46:44.736526Z","shell.execute_reply.started":"2023-05-03T08:46:11.199703Z","shell.execute_reply":"2023-05-03T08:46:44.735596Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/335 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bff369a8e0f4e77bb4d44e7b808dc6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/4.92M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b3ad0dc44de41faa01af4c7bbc7f47f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/9.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acd3bb4cff3f44c7a4ff06322d3a9de3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/276 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a1a4925473b4e1cbc363696418e5217"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/548 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4db839fb49fa448c9f57b95c148a18fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/3.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2905717ee2c49e796ec557964fb2ef7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b0f353257b54bcfaeabfd052a4e689f"}},"metadata":{}}]},{"cell_type":"code","source":"import random\nimport json\n# Change the path of you data\nwith open(\"/kaggle/input/ml2023spring-hw7/hw7_in-context-learning-examples.json\", \"r\") as f: \n    test = json.load(f)\n\n# K-shot learning \n# Give model K examples to make it achieve better accuracy \n# Note: When K >= 4, CUDA_OUT_OFF_MEMORY may occur\nK = 1\n\n\nquestion_ids = [qa[\"id\"] for qa in test[\"questions\"]]\n\nwith torch.no_grad():\n    for idx, qa in enumerate(test[\"questions\"]):\n        # You can try different prompts\n        prompt = \"從最後一篇文章擷取問題的正確答案\\n\"\n        exist_question_indexs = [question_ids.index(qa[\"id\"])]\n        # K-shot learning: give the model K examples with answers\n        for i in range(K):\n            question_index = question_ids.index(qa[\"id\"])\n            while(question_index in exist_question_indexs): \n                question_index = random.randint(0, len(question_ids) - 1)\n            exist_question_indexs.append(question_index)    \n            paragraph_id = test[\"questions\"][question_index][\"paragraph_id\"]\n            prompt += f'文章：{test[\"paragraphs\"][paragraph_id]}\\n'\n            prompt += f'問題：{test[\"questions\"][question_index][\"question_text\"]}\\n'\n            prompt += f'答案：{test[\"questions\"][question_index][\"answer_text\"]}\\n'\n\n\n        # The final one question without answer\n        paragraph_id = qa[\"paragraph_id\"]\n        prompt += f'文章：{test[\"paragraphs\"][paragraph_id]}\\n'\n        prompt += f'問題：{qa[\"question_text\"]}\\n'\n        prompt += f'答案：'\n        \n        inputs = tokenizer(prompt, add_special_tokens=False, return_tensors=\"pt\") \n        sample = model.generate(**inputs, max_new_tokens = 20)\n        text = tokenizer.decode(sample[0], skip_special_tokens=True)\n        # Note: You can delete this line to see what will happen\n        text = clean_text(text)\n        \n        print(prompt)\n        print(f'正確答案: {qa[\"answer_text\"]}')\n        print(f'模型輸出: {text}')\n        print()","metadata":{"execution":{"iopub.status.busy":"2023-05-03T08:46:55.161407Z","iopub.execute_input":"2023-05-03T08:46:55.161750Z","iopub.status.idle":"2023-05-03T08:47:11.643584Z","shell.execute_reply.started":"2023-05-03T08:46:55.161723Z","shell.execute_reply":"2023-05-03T08:47:11.642630Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"從最後一篇文章擷取問題的正確答案\n文章：廣州是京廣鐵路，廣深鐵路，廣茂鐵路和廣梅鐵路的終點站。2009年底，武廣客運專線投入運營，多機組列車長980公里，最高時速350公里。2011年1月7日，廣珠城際鐵路投入運營，平均時速200公里。廣州鐵路，長途巴士和渡輪直達香港。廣九快速列車從廣州火車東站出發，直達香港紅磡火車站。總長約182公里，行程大約需要兩個小時。每年都有繁忙的教練從香港的不同乘客點接載乘客。在市中心的珠江北岸有一條渡輪線路，河流居民可以直接過河而無需乘坐公共汽車或步行穿過大橋。每天都有往返南沙碼頭和蓮花山碼頭的高速雙體船。渡輪也開往香港中國客運碼頭和港澳客運碼頭。\n問題：廣珠城際鐵路平均每小時可以走多遠？\n答案：200公里\n文章：2010年引入的廣州快速交通運輸系統是世界第二大快速運輸系統。每日載客量可達100萬人次。每小時的客流量峰值高達26,900名乘客，僅次於波哥大的快速交通系統。每10秒有一輛公共汽車，每輛公共汽車在一個方向上行駛350小時。該平台包括橋樑，是世界上最長的國家公共汽車快速運輸系統平台，長度為260米。目前，廣州市的出租車和公交車主要以液化石油氣為燃料，部分公交車採用油電，氣電混合技術。2012年底，一輛LNG燃料公共汽車開始啟動。2014年6月，引入了LNG插電式混合動力公交車取代LPG公交車。2007年1月16日，廣州市政府完全禁止在城市地區駕駛摩托車。違反禁令的機動車將被沒收。廣州市交通局聲稱，禁令的實施導致交通擁堵和車禍大大減少。廣州白雲國際機場位於白雲區與花都區交界處。它於2004年8月5日正式投入運營。它是中國第二繁忙的機場。機場取代了原先位於市中心的舊機場，無法滿足日益增長的航空需求。目前，機場有三個簡易機場，是中國第三個擁有三條跑道的民航機場。比2023年香港國際機場第三條跑道的預計完工時間提前了8年。\n問題：從哪一天開始在廣州市內騎摩托車會被沒收？\n答案：\n正確答案: 2007年1月16日\n模型輸出: 2007年1月16日\n\n從最後一篇文章擷取問題的正確答案\n文章：2010年7月，1000多名市民聚集在地鐵江南西站出口處舉行2010年廣州支持粵語行動請願活動。在收到有關該活動被取消的消息後，他們仍然按計劃時間到達為光復文化發行自己的活動。語音。在內地媒體封鎖消息傳出後，廣州還封鎖了與香港電視有關的新聞片段。8月1日，廣州中央公園的粵語聚會受到當局的強烈阻撓。人們被迫轉移路線。當他們到達北京路南段時，他們被大批警察包圍。參與者被帶到公共汽車並在東山體育場被帶走。它在被質疑和被定罪後被釋放。大陸請願制度允許人們向上級政府部門發出呼籲。廣東省政府經常邀請來自省內各地的請願人來到東風路正門，得到省政府的幫助。在集會時，當局將調動大量警察部隊積極處理他們。在對峙期間，交通通常會很慢，但當地官方媒體從未報導相關報導。2014年初，更重要的請願者是該省的家禽商，由於市場被迫關閉，在禽流感期間遭受了損失，以及隋毛反對茂名石化項目的年輕人。\n問題：2010年廣州撐粵語行動請願活動是在當年的什麼時候舉行的？\n答案：7月\n文章：廣州是京廣鐵路，廣深鐵路，廣茂鐵路和廣梅鐵路的終點站。2009年底，武廣客運專線投入運營，多機組列車長980公里，最高時速350公里。2011年1月7日，廣珠城際鐵路投入運營，平均時速200公里。廣州鐵路，長途巴士和渡輪直達香港。廣九快速列車從廣州火車東站出發，直達香港紅磡火車站。總長約182公里，行程大約需要兩個小時。每年都有繁忙的教練從香港的不同乘客點接載乘客。在市中心的珠江北岸有一條渡輪線路，河流居民可以直接過河而無需乘坐公共汽車或步行穿過大橋。每天都有往返南沙碼頭和蓮花山碼頭的高速雙體船。渡輪也開往香港中國客運碼頭和港澳客運碼頭。\n問題：廣珠城際鐵路平均每小時可以走多遠？\n答案：\n正確答案: 200公里\n模型輸出: 200公里。\n\n從最後一篇文章擷取問題的正確答案\n文章：廣州是中國最重要的開放城市之一。作為對外貿易的窗口，有許多來自北美，日本，韓國，歐洲，中東，非洲等國的外國人，尤其是非洲黑人。廣州甚至被稱為“第三世界”。首都主要位於廣州市環市東路秀山大廈，淘金路，花園酒店，建馬路和劍馬路;天河北路，體育東路，天河路，龍口西路，臨河中路等公司辦公室;番禺區的一些物業，如克利福德新村和麗江花園，也有更多的外國人。據廣州市公安局出入境管理支隊2014年10月統計，在廣州登記的外國人有11.8萬人，其中臨時居民71,000人，居住半年以上的47,000名永久居民。其中大多數是來自非洲的4,000多名註冊外國人;來自阿拉伯國家和地區的約1萬人雖然沒有大規模的定居點，但經常出現在小北 - 登封地區的商業區進行商業活動。在荔景街也有更多的韓國人。2013年，有超過15萬名乘客進出廣州港，其中54萬人來自非洲國家。他們中的大多數人來往於同一個人，主要是因為簽證允許短期逗留或商業需求而不是移民。近年來，廣州黑人一直捲入廣州的一些公安問題，引起廣州市民的一些不滿。針對“三非”問題，公安機關設立了四個涉外綜合執法隊伍，翻譯志願服務隊伍，調查“三個非外國”外國專業隊和四個外國隊 - 相關管理基層服務隊伍;自年初以來，廣州在200多名外國人社區建立了75個“外國人管理服務工作站”，並大力打擊“三非”外國人。它已經取得了成果。\n問題：被稱為「第三世界首都」的城市哪種外籍人士最多？\n答案：非洲黑人\n文章：自古以來，廣州一直是華南地區的著名商人，擁有2000多年的開放貿易歷史。20世紀70年代末中國大陸改革開放後，廣州經濟發展迅速。2010年，全市地區生產總值10604.48億元，同比增長13％。它成為僅次於上海和北京的第三個進入“萬億元俱樂部”國內生產總值的城市。這也是第一個超過一萬億的經濟總量。首都。根據國務院2005年發布的報告，廣州成為中國第一個進入“發達”狀態的城市。2012年9月，廣州南沙新區獲批，成為第六個國家級開放開發新區。2015年，廣州GDP達到1810.41億元，人均GDP達到138,377.05元。國內生產總值是中國的第三位，人均國內生產總值與西班牙相當。購買力平價水平與發達國家相當。\n問題：進入國內生產總值「萬億元俱樂部」的城市第三個為？\n答案：\n正確答案: 廣州\n模型輸出: 廣州\n\n從最後一篇文章擷取問題的正確答案\n文章：六朝時期，廣州的對外貿易相當繁榮，外國海商“長期停留在廣州尋求利潤”。在隋唐時期，廣州的對外貿易發展到了頂峰。作為唐代唯一的城市，外國人口占全市人口的30％以上，成為中國對外貿易的核心。在元朝短暫的沉寂之後，廣州和明清在廣州重新出現。在清代的一站式貿易政策下，它成為當時中國唯一的外國港口，成為具有壟斷地位的國家商業中心。上海開放後，隨著國家的逐步開放，廣州逐漸失去了對外貿易中心的地位，但它仍然是全國最重要的商業城市。改革開放後，廣州百貨業蓬勃發展。 20世紀80年代，友誼商店專門為香港，澳門和外國客人銷售電器和其他進口商品。西湖路照明夜市是平民的熱門目的地。南樓位於江西路沿線，已成為華南地區最大的綜合性百貨商場。它還在大陸創建了第一家24小時便利店，華夏百貨商店成立了人民南商圈。延伸，後者於1995年建立了一條步行街。新大新公司和廣州百貨在北京路的業務也蓬勃發展，這條路也成了一條步行街。超市Wankelong於1996年在廣州開設了第一家分店，然後廣州人熟悉的超市和便利店品牌，如香港百佳超市進入廣州。由於交通壓力和天河新區的發展，20世紀90年代末人民南方商業圈開始衰落，成為電子服裝批發的集散地，城市商業區遷至東部新區。21世紀，以天河城，天環廣場，鄭家廣場，太古彙和萬靈彙為代表的天河路商業區已成為廣州的中心商業區。自1957年以來，中國出口商品交易會每年春秋兩季舉辦。目前，每個展覽都吸引了來自世界各地的20多萬商家。這是中國規模最大，時間最長，最成功的國際展覽會。隨著琶洲展館三期工程的建設，展覽能力躍居世界前列。\n問題：唐朝的對外貿易核心在哪？\n答案：廣州\n文章：廣州的雨水豐富，土地肥沃。市區曾經擁有非常大的農業用地。兩千年前有水稻種植記錄。宋代的廣州是中國最大的大米市場之一，蔬菜，水果，甘蔗和鮮花也很有名。由於長期以來一直是國家港口，廣州引進了各種優質作物品種。二十世紀上半葉，由於外國農產品的入侵和戰爭的影響，廣州的農業增長放緩。在20世紀50年代中期，農業生產急劇增加，但是後來的“大躍進”和隨後的文化大革命嚴重打擊了農業生產。改革開放以來，隨著廣州作為一個大城市的發展，服務城市的郊區農業模式逐步形成。政府的政策是為城市服務，豐富農民和人民，並鼓勵出口收入。具體措施是降低糧食生產比例，增加水果，蔬菜等農副產品比重，努力發展林業，畜牧業和漁業。廣州的農業產值大幅增加，但隨著第二，三產業的快速增長，農業的經濟份額逐漸下降。1978年，廣州農業總產值佔地區生產總值的11.67％，1990年下降到8.05％。 2015年，廣州農業總產值為413億元，佔地區GDP的2.28％。城市發展和工業化進程也造成了農村勞動力的大量流失和農地的迅速衰退。清光緒五年間，廣州登記的天山山池總面積為10662.3萬畝。1990年，耕地面積為247萬畝，2006年減少到158.3萬畝。目前，廣州農產品種植地主要分佈在白雲，花都，番禺，南沙，從化和增城。1990年，農業人口為252.87萬，2008年為792,200。廣州最著名的農業特產包括梧桐五秀和各種熱帶水果，如荔枝，香蕉，木瓜和菠蘿。\n問題：1978年的廣州市農業生產總值到了哪一年占地區生總值減少了約9.39%？\n答案：\n正確答案: 2015\n模型輸出: 1990年\n\n從最後一篇文章擷取問題的正確答案\n文章：廣州是中國最重要的開放城市之一。作為對外貿易的窗口，有許多來自北美，日本，韓國，歐洲，中東，非洲等國的外國人，尤其是非洲黑人。廣州甚至被稱為“第三世界”。首都主要位於廣州市環市東路秀山大廈，淘金路，花園酒店，建馬路和劍馬路;天河北路，體育東路，天河路，龍口西路，臨河中路等公司辦公室;番禺區的一些物業，如克利福德新村和麗江花園，也有更多的外國人。據廣州市公安局出入境管理支隊2014年10月統計，在廣州登記的外國人有11.8萬人，其中臨時居民71,000人，居住半年以上的47,000名永久居民。其中大多數是來自非洲的4,000多名註冊外國人;來自阿拉伯國家和地區的約1萬人雖然沒有大規模的定居點，但經常出現在小北 - 登封地區的商業區進行商業活動。在荔景街也有更多的韓國人。2013年，有超過15萬名乘客進出廣州港，其中54萬人來自非洲國家。他們中的大多數人來往於同一個人，主要是因為簽證允許短期逗留或商業需求而不是移民。近年來，廣州黑人一直捲入廣州的一些公安問題，引起廣州市民的一些不滿。針對“三非”問題，公安機關設立了四個涉外綜合執法隊伍，翻譯志願服務隊伍，調查“三個非外國”外國專業隊和四個外國隊 - 相關管理基層服務隊伍;自年初以來，廣州在200多名外國人社區建立了75個“外國人管理服務工作站”，並大力打擊“三非”外國人。它已經取得了成果。\n問題：被稱為「第三世界首都」的城市哪種外籍人士最多？\n答案：非洲黑人\n文章：中華人民共和國成立後的工業國有化。在20世紀50年代和60年代，該行業復甦，但文化大革命再次嚴重影響了廣州的工業生產，工業發展放緩。然而，在政府的支持下，仍然建立了重工業體系。1975年以後，政局改善，政府大力支持以日用品為主的輕工業，廣州工業進入快速增長期。20世紀80年代末，廣州主要發展第三產業，工業產值比重下降。從20世紀90年代到2000年，廣州市政府試圖改變行業構成，增加對重工業的支持。汽車，石化，電子信息產品的製造已成為廣州的三大支柱產業。廣州的工業總產值在全省排名第三。其中，汽車工業成功吸引了日本三大汽車製造企業投資建廠，使廣州成為近幾年來中國重要的汽車生產基地之一。江南最大的廣州晨天汽車零部件市場位於郊區，廣州也是國內。生產日本汽車最多的城市。\n問題：中華人民共和國成立後由於什麼事件廣州的工業發展又減慢？\n答案：\n正確答案: 文化大革命\n模型輸出: 文化大革命\n\n從最後一篇文章擷取問題的正確答案\n文章：廣州的雨水豐富，土地肥沃。市區曾經擁有非常大的農業用地。兩千年前有水稻種植記錄。宋代的廣州是中國最大的大米市場之一，蔬菜，水果，甘蔗和鮮花也很有名。由於長期以來一直是國家港口，廣州引進了各種優質作物品種。二十世紀上半葉，由於外國農產品的入侵和戰爭的影響，廣州的農業增長放緩。在20世紀50年代中期，農業生產急劇增加，但是後來的“大躍進”和隨後的文化大革命嚴重打擊了農業生產。改革開放以來，隨著廣州作為一個大城市的發展，服務城市的郊區農業模式逐步形成。政府的政策是為城市服務，豐富農民和人民，並鼓勵出口收入。具體措施是降低糧食生產比例，增加水果，蔬菜等農副產品比重，努力發展林業，畜牧業和漁業。廣州的農業產值大幅增加，但隨著第二，三產業的快速增長，農業的經濟份額逐漸下降。1978年，廣州農業總產值佔地區生產總值的11.67％，1990年下降到8.05％。 2015年，廣州農業總產值為413億元，佔地區GDP的2.28％。城市發展和工業化進程也造成了農村勞動力的大量流失和農地的迅速衰退。清光緒五年間，廣州登記的天山山池總面積為10662.3萬畝。1990年，耕地面積為247萬畝，2006年減少到158.3萬畝。目前，廣州農產品種植地主要分佈在白雲，花都，番禺，南沙，從化和增城。1990年，農業人口為252.87萬，2008年為792,200。廣州最著名的農業特產包括梧桐五秀和各種熱帶水果，如荔枝，香蕉，木瓜和菠蘿。\n問題：1978年的廣州市農業生產總值到了哪一年占地區生總值減少了約9.39%？\n答案：2015\n文章：六朝時期，廣州的對外貿易相當繁榮，外國海商“長期停留在廣州尋求利潤”。在隋唐時期，廣州的對外貿易發展到了頂峰。作為唐代唯一的城市，外國人口占全市人口的30％以上，成為中國對外貿易的核心。在元朝短暫的沉寂之後，廣州和明清在廣州重新出現。在清代的一站式貿易政策下，它成為當時中國唯一的外國港口，成為具有壟斷地位的國家商業中心。上海開放後，隨著國家的逐步開放，廣州逐漸失去了對外貿易中心的地位，但它仍然是全國最重要的商業城市。改革開放後，廣州百貨業蓬勃發展。 20世紀80年代，友誼商店專門為香港，澳門和外國客人銷售電器和其他進口商品。西湖路照明夜市是平民的熱門目的地。南樓位於江西路沿線，已成為華南地區最大的綜合性百貨商場。它還在大陸創建了第一家24小時便利店，華夏百貨商店成立了人民南商圈。延伸，後者於1995年建立了一條步行街。新大新公司和廣州百貨在北京路的業務也蓬勃發展，這條路也成了一條步行街。超市Wankelong於1996年在廣州開設了第一家分店，然後廣州人熟悉的超市和便利店品牌，如香港百佳超市進入廣州。由於交通壓力和天河新區的發展，20世紀90年代末人民南方商業圈開始衰落，成為電子服裝批發的集散地，城市商業區遷至東部新區。21世紀，以天河城，天環廣場，鄭家廣場，太古彙和萬靈彙為代表的天河路商業區已成為廣州的中心商業區。自1957年以來，中國出口商品交易會每年春秋兩季舉辦。目前，每個展覽都吸引了來自世界各地的20多萬商家。這是中國規模最大，時間最長，最成功的國際展覽會。隨著琶洲展館三期工程的建設，展覽能力躍居世界前列。\n問題：唐朝的對外貿易核心在哪？\n答案：\n正確答案: 廣州\n模型輸出: 廣州\n\n從最後一篇文章擷取問題的正確答案\n文章：清朝末期，廣州爆發了幾次武裝起義，所有起義都以失敗告終。1911年10月10日武昌起義後，廣東省獨立。 11月10日，軍政府成立，胡漢民當選為州長。12月初，廣東省臨時省委成立。據宣布，21歲以上的廣東國民有選舉權和被選舉權。議會由120名成員組成，其中包括20名聯盟成員，21名軍團協會代表，12名海外華人代表以及師生代表。 9人，1名“自治團體”代表，57名來自各地區的代表。其中，女議員必須佔據​​10人，中國女性是第一個參與政治的人。1913年4月27日，廣東省議會成立，羅曉峰被任命為議長。1918年10月，廣州市政府辦公室成立。1921年2月，廣州正式成立，廣州市政府成立，孫克被任命為市長。1917年至1922年間，孫中山和西南各省議會代表兩次在廣州建立了法國政府。1925年，孫中山去世。同年7月，廣州國民政府成立。 7月4日，廣州市政府成立，實行市委製度。吳朝書被任命為市委主席。1929年，廣州實施了市長制。1949年10月14日，中國人民解放軍佔領了廣州。 28日，廣州軍事管理委員會成立。葉劍英是董事長。目前，廣州實行了中華人民共和國政治體制下的人民代表大會制度。市政府在廣州市委員會的領導下運作，政府駐留在越秀區。廣州省是廣東省省會，廣東省人民政府駐地。\n問題：廣東省因為哪一場戰爭而獨立？\n答案：武昌起義\n文章：自古以來，廣州基本上就是嶺南的政治中心。秦朝末年是南嶽的首都，漢朝征服了南嶽州，後來成為南海縣。漢末遷至龍灣和古代大壩。在三國時期，吳國布將縣長遷回番禺，後來成立了膠州府。廣州和廣州轉移後，它是廣州的州長。晉南，南北朝使用南海縣，番禺由縣統治。文帝皇帝廢棄了南海縣，成立了廣州總務辦公室，後來改為杜都政府。唐代分為十個民族，其中嶺南大智學院位於廣州。862年，嶺南分為東二西路，廣州分為嶺南東路。五代時期，廣州是南漢的首都。宋初，嶺南路重建，廣州是行政中心。明代廣州是廣州阜城，隸屬番禺縣和南海縣。清軍佔領中原後，南明邵武帝朱熹在廣州建成，很快南明就死了。清代廣州成為廣東省，廣州阜城，番禺和南海縣。廣東省和廣西省，廣東省屯門省長，廣東省屯門局長，廣東省檢察院都在廣州市南海縣境內。1858年，英法聯軍攻占廣州，廣東省長投降，外國人委員會成立，廣東省政府成為政府。\n問題：南明的首都在哪？\n答案：\n正確答案: 廣州\n模型輸出: 廣\n\n從最後一篇文章擷取問題的正確答案\n文章：作為現代革命的發祥地之一，廣州是中華民國以來中國社會運動的中心之一。每次全國社會運動都有廣州人民的回應和參與。一個相對大規模的社會運動集中在廣州，這是1925年至1926年在廣州和香港舉行的首次省和港口罷工。1989年，廣州市民發起了支持天安門民主運動的運動。數百萬人聚集在海珠廣場，聚集在廣州解放紀念碑周圍。在早期，廣州媒體支持“愛國運動”的名稱。流血事件發生後，民眾發生騷亂，省市政府機構遭到襲擊，所有廣州媒體都被立即禁止，這也乾擾並封鎖了香港電視台。省政府派出軍隊和警察鎮壓遊行群眾，大量參與者潛入香港，台灣和海外。所有企事業單位也派人檢查各部門員工是否組織或參加了集會。在1999年的全國反美活動中，數十萬人和學生在該市舉行示威活動，抗議北約轟炸中國駐南斯拉夫大使館。與此同時，美國駐廣州領事館也受到一些激進示威者的破壞。廣州媒體在此過程中也紛紛效仿，但沒有提到美國駐廣州領事館受損的情況。2005年，主要道路上發生了數十萬起反日遊行。然而，在廣州的封鎖中，大學和中學也禁止學生遊行，否則他們就開除了學業。\n問題：省港大罷工是除了廣州以外還有哪個地區參與？\n答案：香港\n文章：清朝末期，廣州爆發了幾次武裝起義，所有起義都以失敗告終。1911年10月10日武昌起義後，廣東省獨立。 11月10日，軍政府成立，胡漢民當選為州長。12月初，廣東省臨時省委成立。據宣布，21歲以上的廣東國民有選舉權和被選舉權。議會由120名成員組成，其中包括20名聯盟成員，21名軍團協會代表，12名海外華人代表以及師生代表。 9人，1名“自治團體”代表，57名來自各地區的代表。其中，女議員必須佔據​​10人，中國女性是第一個參與政治的人。1913年4月27日，廣東省議會成立，羅曉峰被任命為議長。1918年10月，廣州市政府辦公室成立。1921年2月，廣州正式成立，廣州市政府成立，孫克被任命為市長。1917年至1922年間，孫中山和西南各省議會代表兩次在廣州建立了法國政府。1925年，孫中山去世。同年7月，廣州國民政府成立。 7月4日，廣州市政府成立，實行市委製度。吳朝書被任命為市委主席。1929年，廣州實施了市長制。1949年10月14日，中國人民解放軍佔領了廣州。 28日，廣州軍事管理委員會成立。葉劍英是董事長。目前，廣州實行了中華人民共和國政治體制下的人民代表大會制度。市政府在廣州市委員會的領導下運作，政府駐留在越秀區。廣州省是廣東省省會，廣東省人民政府駐地。\n問題：廣東省因為哪一場戰爭而獨立？\n答案：\n正確答案: 武昌起義\n模型輸出: 廣州爆發了幾次武裝起義,所有起義都以失敗告終\n\n從最後一篇文章擷取問題的正確答案\n文章：六朝時期，廣州的對外貿易相當繁榮，外國海商“長期停留在廣州尋求利潤”。在隋唐時期，廣州的對外貿易發展到了頂峰。作為唐代唯一的城市，外國人口占全市人口的30％以上，成為中國對外貿易的核心。在元朝短暫的沉寂之後，廣州和明清在廣州重新出現。在清代的一站式貿易政策下，它成為當時中國唯一的外國港口，成為具有壟斷地位的國家商業中心。上海開放後，隨著國家的逐步開放，廣州逐漸失去了對外貿易中心的地位，但它仍然是全國最重要的商業城市。改革開放後，廣州百貨業蓬勃發展。 20世紀80年代，友誼商店專門為香港，澳門和外國客人銷售電器和其他進口商品。西湖路照明夜市是平民的熱門目的地。南樓位於江西路沿線，已成為華南地區最大的綜合性百貨商場。它還在大陸創建了第一家24小時便利店，華夏百貨商店成立了人民南商圈。延伸，後者於1995年建立了一條步行街。新大新公司和廣州百貨在北京路的業務也蓬勃發展，這條路也成了一條步行街。超市Wankelong於1996年在廣州開設了第一家分店，然後廣州人熟悉的超市和便利店品牌，如香港百佳超市進入廣州。由於交通壓力和天河新區的發展，20世紀90年代末人民南方商業圈開始衰落，成為電子服裝批發的集散地，城市商業區遷至東部新區。21世紀，以天河城，天環廣場，鄭家廣場，太古彙和萬靈彙為代表的天河路商業區已成為廣州的中心商業區。自1957年以來，中國出口商品交易會每年春秋兩季舉辦。目前，每個展覽都吸引了來自世界各地的20多萬商家。這是中國規模最大，時間最長，最成功的國際展覽會。隨著琶洲展館三期工程的建設，展覽能力躍居世界前列。\n問題：唐朝的對外貿易核心在哪？\n答案：廣州\n文章：作為現代革命的發祥地之一，廣州是中華民國以來中國社會運動的中心之一。每次全國社會運動都有廣州人民的回應和參與。一個相對大規模的社會運動集中在廣州，這是1925年至1926年在廣州和香港舉行的首次省和港口罷工。1989年，廣州市民發起了支持天安門民主運動的運動。數百萬人聚集在海珠廣場，聚集在廣州解放紀念碑周圍。在早期，廣州媒體支持“愛國運動”的名稱。流血事件發生後，民眾發生騷亂，省市政府機構遭到襲擊，所有廣州媒體都被立即禁止，這也乾擾並封鎖了香港電視台。省政府派出軍隊和警察鎮壓遊行群眾，大量參與者潛入香港，台灣和海外。所有企事業單位也派人檢查各部門員工是否組織或參加了集會。在1999年的全國反美活動中，數十萬人和學生在該市舉行示威活動，抗議北約轟炸中國駐南斯拉夫大使館。與此同時，美國駐廣州領事館也受到一些激進示威者的破壞。廣州媒體在此過程中也紛紛效仿，但沒有提到美國駐廣州領事館受損的情況。2005年，主要道路上發生了數十萬起反日遊行。然而，在廣州的封鎖中，大學和中學也禁止學生遊行，否則他們就開除了學業。\n問題：省港大罷工是除了廣州以外還有哪個地區參與？\n答案：\n正確答案: 香港\n模型輸出: 廣州\n\n從最後一篇文章擷取問題的正確答案\n文章：多年來，廣州當局積極開展各種政府項目。自1990年以來，廣州參與了“創建國家健康城市”，並於2008年成功獲得該獎項。1998年，“創建全國文明城市”開始，2011年取得了成功。在“創造健康”期間，廣州市的衛生狀況得到了明顯改善，如清理城市衛生角，城中村，污水處理和食品安全管理。一些官員說，“創造健康”只是一個起點，沒有盡頭。 “創造”是“人心的項目”。關於在這些項目，形式主義和騷亂上花費的巨額公共資金，還有媒體問題和批評。多年來在這兩個項目上花費的公共資金數量已達數十億，這使得計算難度很大。在廣州亞運會之際，廣州市政府在2010年之前開展了大量的城市建設項目，包括：改造多條道路和人行道，重新鋪設地磚和花卉基地;和許多新老天橋的灰白色油漆;大多數街道拱廊和歷史建築的改造;正面建築的屋頂均勻配備紅色塑料屋頂，外牆經過翻新，配備空調壓縮機蓋和照明。然而，有些人質疑，在亞運會期間，許多精心準備的道路被強行翻新，當街邊商場和歷史建築被翻新時，原有的歷史和藝術感被摧毀。許多小偷參與了街道上數千棟建築的統一改造。通過腳手架，數百戶家庭被盜。項目完成後不久，揭露了涉及使用劣質材料的事件，這也遭到許多公民的批評。作為中國的中心城市之一，廣州的醫療保險和便利服務覆蓋了整個城市。大多數市政公園和所有公共廁所也是免費的。\n問題：廣州市於什麼時候獲得從1990年開始參與的「創衛」？\n答案：2008年\n文章：公元前214年，秦始皇出兵徵兵嶺南，南海縣任仁軍在梵山和廬山建造了番禺城。這是廣州最初建成時的名稱，已在中華民國使用。公元226年和246年，吳國將膠州分為兩部分：“廣州”和“廣州”。合浦以南仍被稱為“城洲”，合浦以北被稱為國務院芷芝部的“廣州”。 “廣州”包括南海，蒼梧，榆林和合浦四個縣。這是兩國分裂的開始，也是“廣州”這個名稱的開頭，但廣州當時只提到了一個管轄區，相當於現在的廣東省。自唐代以來，廣州地區已成立為光復的起源。它於1921年正式成立，番禺市首次被評為廣州。廣州又被稱為“五羊城”，“陽城”和“遂城”，簡稱“隋”。這些別名來自五羊的傳說：有五個神仙穿著五色衣服，騎著五色羊。拿著“一根六根”的穀物來到廣州市，在把耳朵送到城市後，他們飛走了。五隻羊變成了石頭。因此，廣州也被稱為“五陽城”和“遂城”。因為它位於亞熱帶地區，氣候溫暖​​，適合種植花卉。此外，廣州人喜愛種花，買花，送花。花卉市場每年繁榮，廣州享有華城的美譽。從唐代開始，廣州就是廣州的首府。清朝以後，它是廣東省的省會。因此，省會成為廣州廣東人的總稱。在古代，廣州也有南武市，楚庭，番禺和南海的稱號。\n問題：番禺城這一個名子一直用到哪一年才改名為廣州市？\n答案：\n正確答案: 1921\n模型輸出: 1918年\n\n從最後一篇文章擷取問題的正確答案\n文章：自古以來，廣州基本上就是嶺南的政治中心。秦朝末年是南嶽的首都，漢朝征服了南嶽州，後來成為南海縣。漢末遷至龍灣和古代大壩。在三國時期，吳國布將縣長遷回番禺，後來成立了膠州府。廣州和廣州轉移後，它是廣州的州長。晉南，南北朝使用南海縣，番禺由縣統治。文帝皇帝廢棄了南海縣，成立了廣州總務辦公室，後來改為杜都政府。唐代分為十個民族，其中嶺南大智學院位於廣州。862年，嶺南分為東二西路，廣州分為嶺南東路。五代時期，廣州是南漢的首都。宋初，嶺南路重建，廣州是行政中心。明代廣州是廣州阜城，隸屬番禺縣和南海縣。清軍佔領中原後，南明邵武帝朱熹在廣州建成，很快南明就死了。清代廣州成為廣東省，廣州阜城，番禺和南海縣。廣東省和廣西省，廣東省屯門省長，廣東省屯門局長，廣東省檢察院都在廣州市南海縣境內。1858年，英法聯軍攻占廣州，廣東省長投降，外國人委員會成立，廣東省政府成為政府。\n問題：南明的首都在哪？\n答案：廣州\n文章：2010年7月，1000多名市民聚集在地鐵江南西站出口處舉行2010年廣州支持粵語行動請願活動。在收到有關該活動被取消的消息後，他們仍然按計劃時間到達為光復文化發行自己的活動。語音。在內地媒體封鎖消息傳出後，廣州還封鎖了與香港電視有關的新聞片段。8月1日，廣州中央公園的粵語聚會受到當局的強烈阻撓。人們被迫轉移路線。當他們到達北京路南段時，他們被大批警察包圍。參與者被帶到公共汽車並在東山體育場被帶走。它在被質疑和被定罪後被釋放。大陸請願制度允許人們向上級政府部門發出呼籲。廣東省政府經常邀請來自省內各地的請願人來到東風路正門，得到省政府的幫助。在集會時，當局將調動大量警察部隊積極處理他們。在對峙期間，交通通常會很慢，但當地官方媒體從未報導相關報導。2014年初，更重要的請願者是該省的家禽商，由於市場被迫關閉，在禽流感期間遭受了損失，以及隋毛反對茂名石化項目的年輕人。\n問題：2010年廣州撐粵語行動請願活動是在當年的什麼時候舉行的？\n答案：\n正確答案: 7月\n模型輸出: 2010年7月,1000多名市民聚集在地鐵江南西站出口處舉\n\n從最後一篇文章擷取問題的正確答案\n文章：2010年引入的廣州快速交通運輸系統是世界第二大快速運輸系統。每日載客量可達100萬人次。每小時的客流量峰值高達26,900名乘客，僅次於波哥大的快速交通系統。每10秒有一輛公共汽車，每輛公共汽車在一個方向上行駛350小時。該平台包括橋樑，是世界上最長的國家公共汽車快速運輸系統平台，長度為260米。目前，廣州市的出租車和公交車主要以液化石油氣為燃料，部分公交車採用油電，氣電混合技術。2012年底，一輛LNG燃料公共汽車開始啟動。2014年6月，引入了LNG插電式混合動力公交車取代LPG公交車。2007年1月16日，廣州市政府完全禁止在城市地區駕駛摩托車。違反禁令的機動車將被沒收。廣州市交通局聲稱，禁令的實施導致交通擁堵和車禍大大減少。廣州白雲國際機場位於白雲區與花都區交界處。它於2004年8月5日正式投入運營。它是中國第二繁忙的機場。機場取代了原先位於市中心的舊機場，無法滿足日益增長的航空需求。目前，機場有三個簡易機場，是中國第三個擁有三條跑道的民航機場。比2023年香港國際機場第三條跑道的預計完工時間提前了8年。\n問題：從哪一天開始在廣州市內騎摩托車會被沒收？\n答案：2007年1月16日\n文章：多年來，廣州當局積極開展各種政府項目。自1990年以來，廣州參與了“創建國家健康城市”，並於2008年成功獲得該獎項。1998年，“創建全國文明城市”開始，2011年取得了成功。在“創造健康”期間，廣州市的衛生狀況得到了明顯改善，如清理城市衛生角，城中村，污水處理和食品安全管理。一些官員說，“創造健康”只是一個起點，沒有盡頭。 “創造”是“人心的項目”。關於在這些項目，形式主義和騷亂上花費的巨額公共資金，還有媒體問題和批評。多年來在這兩個項目上花費的公共資金數量已達數十億，這使得計算難度很大。在廣州亞運會之際，廣州市政府在2010年之前開展了大量的城市建設項目，包括：改造多條道路和人行道，重新鋪設地磚和花卉基地;和許多新老天橋的灰白色油漆;大多數街道拱廊和歷史建築的改造;正面建築的屋頂均勻配備紅色塑料屋頂，外牆經過翻新，配備空調壓縮機蓋和照明。然而，有些人質疑，在亞運會期間，許多精心準備的道路被強行翻新，當街邊商場和歷史建築被翻新時，原有的歷史和藝術感被摧毀。許多小偷參與了街道上數千棟建築的統一改造。通過腳手架，數百戶家庭被盜。項目完成後不久，揭露了涉及使用劣質材料的事件，這也遭到許多公民的批評。作為中國的中心城市之一，廣州的醫療保險和便利服務覆蓋了整個城市。大多數市政公園和所有公共廁所也是免費的。\n問題：廣州市於什麼時候獲得從1990年開始參與的「創衛」？\n答案：\n正確答案: 2008年\n模型輸出: 2008年\n\n從最後一篇文章擷取問題的正確答案\n文章：公元前214年，秦始皇出兵徵兵嶺南，南海縣任仁軍在梵山和廬山建造了番禺城。這是廣州最初建成時的名稱，已在中華民國使用。公元226年和246年，吳國將膠州分為兩部分：“廣州”和“廣州”。合浦以南仍被稱為“城洲”，合浦以北被稱為國務院芷芝部的“廣州”。 “廣州”包括南海，蒼梧，榆林和合浦四個縣。這是兩國分裂的開始，也是“廣州”這個名稱的開頭，但廣州當時只提到了一個管轄區，相當於現在的廣東省。自唐代以來，廣州地區已成立為光復的起源。它於1921年正式成立，番禺市首次被評為廣州。廣州又被稱為“五羊城”，“陽城”和“遂城”，簡稱“隋”。這些別名來自五羊的傳說：有五個神仙穿著五色衣服，騎著五色羊。拿著“一根六根”的穀物來到廣州市，在把耳朵送到城市後，他們飛走了。五隻羊變成了石頭。因此，廣州也被稱為“五陽城”和“遂城”。因為它位於亞熱帶地區，氣候溫暖​​，適合種植花卉。此外，廣州人喜愛種花，買花，送花。花卉市場每年繁榮，廣州享有華城的美譽。從唐代開始，廣州就是廣州的首府。清朝以後，它是廣東省的省會。因此，省會成為廣州廣東人的總稱。在古代，廣州也有南武市，楚庭，番禺和南海的稱號。\n問題：番禺城這一個名子一直用到哪一年才改名為廣州市？\n答案：1921\n文章：2012年，全市常住人口1283萬人，戶籍人口8230萬，城鎮常住人口778.17萬人，外來人口占36％。廣州是全國人口密度高的城市之一。大多數人口集中在舊城區。四個中心區的平均居民人口密度達到每平方公里18,113人。人口密度最高的越秀區每平方公里達到34,009人，是人口密度最低的從化市的100多倍。自2001年以來，廣州的年出生率一直低於每千人9.6人，自然增長率低於每千人3.4人。公民的預期壽命高於全國平均水平，男性的預期壽命為75.25歲，女性的預期壽命為80.94歲。與此同時，人口老齡化問題也日益突出。截至2008年底，60歲以上老年人口達到1103,500人，佔全市總人口的13.4％。其中，老城區老年人口比例最高，超過18％，預計2030年老年人口將超過200萬。2010年第六次全國人口普查顯示，廣州的男性多於女性。截至2007年底，男女比例為109.46，即每100名女性109.46名男性。男性多於女性573,694。其中，0~14歲年齡組性別失衡嚴重，達到115.62，表明廣州性別比差距將在未來擴大。廣州人口以漢族為主，佔常住人口的98.3％，少數民族人口為219,900人。它屬於55個少數民族。其中，壯族，土家族，苗族，瑤族，回族，滿族，彝族，蒙古族，布依族，朝鮮族等10個民族。廣州經濟的快速發展和高就業機會吸引了大批農民工。 2008年，農民工佔該市人口的40％。如何妥善管理城市管理是市政府的一項艱鉅任務。\n問題：哪邊的人口密度為廣州市的一百倍分之一以下？\n答案：\n正確答案: 從化市\n模型輸出: 廣州市的人口密度是100倍分之一以下。\n\n從最後一篇文章擷取問題的正確答案\n文章：多年來，廣州當局積極開展各種政府項目。自1990年以來，廣州參與了“創建國家健康城市”，並於2008年成功獲得該獎項。1998年，“創建全國文明城市”開始，2011年取得了成功。在“創造健康”期間，廣州市的衛生狀況得到了明顯改善，如清理城市衛生角，城中村，污水處理和食品安全管理。一些官員說，“創造健康”只是一個起點，沒有盡頭。 “創造”是“人心的項目”。關於在這些項目，形式主義和騷亂上花費的巨額公共資金，還有媒體問題和批評。多年來在這兩個項目上花費的公共資金數量已達數十億，這使得計算難度很大。在廣州亞運會之際，廣州市政府在2010年之前開展了大量的城市建設項目，包括：改造多條道路和人行道，重新鋪設地磚和花卉基地;和許多新老天橋的灰白色油漆;大多數街道拱廊和歷史建築的改造;正面建築的屋頂均勻配備紅色塑料屋頂，外牆經過翻新，配備空調壓縮機蓋和照明。然而，有些人質疑，在亞運會期間，許多精心準備的道路被強行翻新，當街邊商場和歷史建築被翻新時，原有的歷史和藝術感被摧毀。許多小偷參與了街道上數千棟建築的統一改造。通過腳手架，數百戶家庭被盜。項目完成後不久，揭露了涉及使用劣質材料的事件，這也遭到許多公民的批評。作為中國的中心城市之一，廣州的醫療保險和便利服務覆蓋了整個城市。大多數市政公園和所有公共廁所也是免費的。\n問題：廣州市於什麼時候獲得從1990年開始參與的「創衛」？\n答案：2008年\n文章：作為中國最大的城市之一，廣州有大量在廣州工作的外國人。然而，“本地人”與“外國人”之間的關係難以協調，也會引發一系列社會問題。由於廣州的能力和就業能力有限，廣州無法容納來自全國各地的農民工。其安全環境已經過嚴格測試。經常發生“兩個盜賊和一個小偷”事件，特別是在城市村莊。根據廣州社會科學院的一項調查，80％的囚犯被逮捕為“局外人”。除了廣東話的光復文化外，外國人和外國方言的排斥增加了廣州市民的仇外心理。近年來，由於政府的大力禁令，廣州的公安管理取得了顯著成效。2014年，廣東省政府明確要求控制廣州和深圳特大城市的人口規模。2008年，農民工佔廣州人口的40％，其中大多數是女性農民工，其中許多人未婚。截至2013年底，廣州註冊人口為686.7萬。根據一定的錯失率，廣州的實際流動人口約為837萬。這些數據已超過常住人口。最新統計數據顯示，廣州常住人口為832萬。其中，白雲區，番禺區和天河區流動人口超過百萬。番禺區有111.7萬戶登記移民，流動戶籍區涉及30個省，自治區，直轄市。在性別比方面，男性略多，佔總數的53.5％;女性佔總數的46.5％。在番禺流動人口中，兒童和老年人的比例極小，15歲以下為0.49％，60歲以上為1.68％，大專以上為10％。\n問題：有百分之多少的犯人被抓的是外地人？\n答案：\n正確答案: 80\n模型輸出: 80%\n\n從最後一篇文章擷取問題的正確答案\n文章：多年來，廣州當局積極開展各種政府項目。自1990年以來，廣州參與了“創建國家健康城市”，並於2008年成功獲得該獎項。1998年，“創建全國文明城市”開始，2011年取得了成功。在“創造健康”期間，廣州市的衛生狀況得到了明顯改善，如清理城市衛生角，城中村，污水處理和食品安全管理。一些官員說，“創造健康”只是一個起點，沒有盡頭。 “創造”是“人心的項目”。關於在這些項目，形式主義和騷亂上花費的巨額公共資金，還有媒體問題和批評。多年來在這兩個項目上花費的公共資金數量已達數十億，這使得計算難度很大。在廣州亞運會之際，廣州市政府在2010年之前開展了大量的城市建設項目，包括：改造多條道路和人行道，重新鋪設地磚和花卉基地;和許多新老天橋的灰白色油漆;大多數街道拱廊和歷史建築的改造;正面建築的屋頂均勻配備紅色塑料屋頂，外牆經過翻新，配備空調壓縮機蓋和照明。然而，有些人質疑，在亞運會期間，許多精心準備的道路被強行翻新，當街邊商場和歷史建築被翻新時，原有的歷史和藝術感被摧毀。許多小偷參與了街道上數千棟建築的統一改造。通過腳手架，數百戶家庭被盜。項目完成後不久，揭露了涉及使用劣質材料的事件，這也遭到許多公民的批評。作為中國的中心城市之一，廣州的醫療保險和便利服務覆蓋了整個城市。大多數市政公園和所有公共廁所也是免費的。\n問題：廣州市於什麼時候獲得從1990年開始參與的「創衛」？\n答案：2008年\n文章：廣州是中國最重要的開放城市之一。作為對外貿易的窗口，有許多來自北美，日本，韓國，歐洲，中東，非洲等國的外國人，尤其是非洲黑人。廣州甚至被稱為“第三世界”。首都主要位於廣州市環市東路秀山大廈，淘金路，花園酒店，建馬路和劍馬路;天河北路，體育東路，天河路，龍口西路，臨河中路等公司辦公室;番禺區的一些物業，如克利福德新村和麗江花園，也有更多的外國人。據廣州市公安局出入境管理支隊2014年10月統計，在廣州登記的外國人有11.8萬人，其中臨時居民71,000人，居住半年以上的47,000名永久居民。其中大多數是來自非洲的4,000多名註冊外國人;來自阿拉伯國家和地區的約1萬人雖然沒有大規模的定居點，但經常出現在小北 - 登封地區的商業區進行商業活動。在荔景街也有更多的韓國人。2013年，有超過15萬名乘客進出廣州港，其中54萬人來自非洲國家。他們中的大多數人來往於同一個人，主要是因為簽證允許短期逗留或商業需求而不是移民。近年來，廣州黑人一直捲入廣州的一些公安問題，引起廣州市民的一些不滿。針對“三非”問題，公安機關設立了四個涉外綜合執法隊伍，翻譯志願服務隊伍，調查“三個非外國”外國專業隊和四個外國隊 - 相關管理基層服務隊伍;自年初以來，廣州在200多名外國人社區建立了75個“外國人管理服務工作站”，並大力打擊“三非”外國人。它已經取得了成果。\n問題：被稱為「第三世界首都」的城市哪種外籍人士最多？\n答案：\n正確答案: 非洲黑人\n模型輸出: 非洲黑人\n\n從最後一篇文章擷取問題的正確答案\n文章：廣州是京廣鐵路，廣深鐵路，廣茂鐵路和廣梅鐵路的終點站。2009年底，武廣客運專線投入運營，多機組列車長980公里，最高時速350公里。2011年1月7日，廣珠城際鐵路投入運營，平均時速200公里。廣州鐵路，長途巴士和渡輪直達香港。廣九快速列車從廣州火車東站出發，直達香港紅磡火車站。總長約182公里，行程大約需要兩個小時。每年都有繁忙的教練從香港的不同乘客點接載乘客。在市中心的珠江北岸有一條渡輪線路，河流居民可以直接過河而無需乘坐公共汽車或步行穿過大橋。每天都有往返南沙碼頭和蓮花山碼頭的高速雙體船。渡輪也開往香港中國客運碼頭和港澳客運碼頭。\n問題：廣珠城際鐵路平均每小時可以走多遠？\n答案：200公里\n文章：長期以來，廣州大部分地方的語言主要是廣東話。因此，廣州還形成了粵語文化，包括粵劇，粵劇，南音和鹹水歌。隨著中國大陸的改革開放，來自城外和廣東省以外的大量人口迅速湧入。在這些移民中，由於不同的省/總統，有許多不同的語言，外國人和來自不同地區的人經常用普通話交流;因此，在許多大型公共場所，普通話是主要語言。在某些情況下，將增加粵語或英語廣播，種族群體仍然使用自己的方言進行相互交流。一些外國人會用當地文化交流，用粵語與當地人交流。如今，隨著普通話的推廣，外國和本地年輕人受長期使用普通話作為教學媒介和普通話的共同環境的影響。目前，廣州的語言交換主要講普通話，但當地人仍然使用廣東話。與交流。雖然普通話自21世紀初以來逐漸佔據主導地位，但它受到當地文化的影響。與此同時，由於香港和澳門的距離較近，以及廣州市民近年來開展了一系列推廣普通話的評論，它已開始保護廣東話，所以廣東話仍有一定的地位。當地。 。\n問題：在廣州許多比較大的公眾場合以甚麼為主要交流語言？\n答案：\n正確答案: 普通話\n模型輸出: 廣州的語言交換主要講普通話,但當地人仍然使用廣東話\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Reference\n1. I modify the sample code from NTU machine learning course","metadata":{"id":"uYNnYHVHX1Rc"}}]}